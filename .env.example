# A2A MCP Server - Configuration
# Copy this file to .env and customize as needed

# ===== DATABASE SETTINGS =====
# PostgreSQL connection string for Prisma
# Format: postgresql://USER:PASSWORD@HOST:PORT/DATABASE?schema=SCHEMA
DATABASE_URL=postgresql://postgres:password@localhost:5432/a2a?schema=public

# Example for local development:
# DATABASE_URL=postgresql://postgres:postgres@localhost:5432/a2a_dev?schema=public

# Example for production (Railway, Render, etc.):
# DATABASE_URL=postgresql://user:pass@host.railway.app:5432/railway?schema=public

# ===== OLLAMA SETTINGS =====
LOCAL_LLM_URL=http://localhost:11434
NODE_ENV=development

# Default models (adjust based on your downloaded models)
DEFAULT_CODE_MODEL=codellama:7b-code
DEFAULT_CHAT_MODEL=llama2:7b-chat
DEFAULT_CREATIVE_MODEL=llama2:13b-chat
DEFAULT_FAST_MODEL=phi3:mini

# ===== SERVER SETTINGS =====
PORT=8787
STREAM_PORT=8787
STREAM_HOST=0.0.0.0
METRICS_PORT=8787

# ===== PERFORMANCE TUNING =====
MAX_CONCURRENCY=5
MAX_QUEUE_SIZE=10000
LOG_LEVEL=info

# Enable streaming for real-time responses
ENABLE_STREAMING=true

# ===== OLLAMA PERFORMANCE =====
# Uncomment and adjust based on your hardware
# OLLAMA_NUM_PARALLEL=4
# OLLAMA_MAX_LOADED_MODELS=2
# OLLAMA_FLASH_ATTENTION=1

# ===== WARP-SPECIFIC OLLAMA CONFIGURATION =====
# Enable Warp integration for Ollama
WARP_OLLAMA_ENABLED=true

# Enable GPU monitoring for Warp
WARP_GPU_MONITORING=true

# GPU acceleration layers for Ollama (adjust based on VRAM)
# -1 = auto-detect, 0 = CPU only, >0 = number of layers offloaded to GPU
OLLAMA_GPU_LAYERS=-1

# Preload models on Warp startup for faster initial responses
WARP_MODEL_PRELOAD=true

# Warp-specific performance tuning
# Number of parallel requests Warp can handle
WARP_MAX_PARALLEL_REQUESTS=3

# Warp model cache size (in MB)
WARP_MODEL_CACHE_SIZE=4096

# Enable Warp's optimized context handling
WARP_OPTIMIZED_CONTEXT=true

# Warp batch size for inference
WARP_BATCH_SIZE=512

# Warp thread count for CPU inference
WARP_NUM_THREADS=8

# Enable Warp's memory-efficient attention mechanism
WARP_EFFICIENT_ATTENTION=true

# Warp model warm-up on startup
WARP_WARMUP_ENABLED=true

# GPU device selection (0 = first GPU, 1 = second GPU, etc.)
WARP_GPU_DEVICE=0

# Warp tensor split for multi-GPU setups (comma-separated ratios)
# WARP_TENSOR_SPLIT=0.5,0.5

# Enable Warp's low-memory mode
WARP_LOW_MEMORY_MODE=false

# Warp context window size
WARP_CONTEXT_SIZE=4096

# Warp prediction batch size
WARP_PREDICT_BATCH_SIZE=128

# ===== MEMORY & STORAGE =====
# Memory limits for agent execution
MAX_EXECUTION_TIME=60000
MAX_FILE_SIZE=10485760

# Memory system configuration
MEMORY_DIR=./data/agent-memory
MAX_MEMORIES_PER_AGENT=10000

# ===== ANALYTICS =====
# Keep event history for analytics
MAX_EVENT_HISTORY=100000
ANALYTICS_CLEANUP_INTERVAL=86400000

# ===== SECURITY =====
# Optional: Add authentication token for streaming
# STREAM_TOKEN=your-secure-token-here

# ===== DEPLOYMENT =====
# For production deployments
# NODE_OPTIONS=--max-old-space-size=2048

# ===== CUSTOM CODE RAG SETTINGS =====
# Voyage AI API key for code embeddings
VOYAGE_API_KEY=your-voyage-api-key-here

# Vector database path for code RAG
RAG_DB_PATH=./data/vector_db

# Indexing settings
RAG_MAX_CHUNK_TOKENS=15000
RAG_REINDEX_INTERVAL=weekly
