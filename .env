# A2A MCP Server - Ollama Configuration
# Copy this file to .env and customize as needed

# ===== OLLAMA SETTINGS =====
LOCAL_LLM_URL=http://localhost:11434
NODE_ENV=development

# Default models (adjust based on your downloaded models)
DEFAULT_CODE_MODEL=codellama:7b-code
DEFAULT_CHAT_MODEL=llama2:7b-chat
DEFAULT_CREATIVE_MODEL=llama2:13b-chat
DEFAULT_FAST_MODEL=phi3:mini

# ===== SERVER SETTINGS =====
PORT=8787
STREAM_PORT=8787
STREAM_HOST=0.0.0.0
METRICS_PORT=9090

# ===== PERFORMANCE TUNING =====
MAX_CONCURRENCY=5
MAX_QUEUE_SIZE=10000
LOG_LEVEL=info

# Enable streaming for real-time responses
ENABLE_STREAMING=true

# ===== OLLAMA PERFORMANCE =====
# Uncomment and adjust based on your hardware
# OLLAMA_NUM_PARALLEL=4
# OLLAMA_MAX_LOADED_MODELS=2
# OLLAMA_FLASH_ATTENTION=1

# ===== MEMORY & STORAGE =====
# Memory limits for agent execution
MAX_EXECUTION_TIME=60000
MAX_FILE_SIZE=10485760

# Memory system configuration
MEMORY_DIR=./data/agent-memory
MAX_MEMORIES_PER_AGENT=10000

# ===== ANALYTICS =====
# Keep event history for analytics
MAX_EVENT_HISTORY=100000
ANALYTICS_CLEANUP_INTERVAL=86400000

# ===== SECURITY =====
# Optional: Add authentication token for streaming
# STREAM_TOKEN=your-secure-token-here

# ===== DEPLOYMENT =====
# For production deployments
# NODE_OPTIONS=--max-old-space-size=2048